#!/usr/bin/env python

import hydra
from omegaconf import DictConfig, OmegaConf 

import logging
import socket
from collections import ChainMap
import importlib

from sbivibm.runs.evaluate import store_results, evaluate_metric, do_not_evaluate_metric
from sbivibm.utils import get_tasks

import torch 
import random 
import numpy as np


import time
import sbibm


@hydra.main(config_path="../config", config_name="config.yaml")
def run_benchmark(cfg : DictConfig) -> None:
    print(OmegaConf.to_yaml(cfg))

    # Logging
    log = logging.getLogger(__name__)
    log.info(OmegaConf.to_yaml(cfg))
    log.info(f"sbibm version: {sbibm.__version__}")
    log.info(f"Hostname: {socket.gethostname()}")

    # Seeding
    seed = cfg.seed
    torch.manual_seed(seed)
    random.seed(seed)
    np.random.seed(seed)
    log.info(f"Random seed: {seed}")

    # Running algorithm
    bm = importlib.import_module(cfg.method.run)

    task = get_tasks(cfg.task.name)
    log.info("Starting run")
    start_time = time.time()
    algorithm_params = dict(cfg.method.params)
    log.info(algorithm_params)

    if "vi" in cfg.method.method_name:
        log.info(cfg.task.vi_parameters)

    if cfg.method.run == "sbivibm.runs.benchmark_snlvi_record_rounds":
        posteriors, samples, _, single_round_results  = bm.run(task, num_observation=cfg.task.num_observation, num_samples=cfg.task.num_posterior_samples, num_simulations=cfg.task.num_simulations, vi_parameters=dict(cfg.task.vi_parameters), **cfg.method.params,)
    else:
        posteriors, samples, _  = bm.run(task, num_observation=cfg.task.num_observation, num_samples=cfg.task.num_posterior_samples, num_simulations=cfg.task.num_simulations, vi_parameters=dict(cfg.task.vi_parameters), **cfg.method.params,)
        single_round_results = None

    end_time = time.time()
    runtime = end_time - start_time 
    log.info(f"Finished run in {np.round(runtime, 2)} seconds")

    
    # Storring results
    if cfg.store_results:
        log.info("Storing results ...")
        result_folder = store_results(cfg.name, task, posteriors, samples, cfg.compute_predictives, single_round_results=single_round_results)
        log.info(f"Saved results in {result_folder}")

    # Evaluate metrics
    if cfg.evaluate_metrics:
        assert cfg.store_results, "You must store the reults to evaluate the metrics"
        log.info("Evaluating metrics ...")
        df_metrics = evaluate_metric(task, cfg, algorithm_params, result_folder, runtime)
        log.info("Finished evaluation")
        log.info(f"Metrics:\n{df_metrics.transpose().to_string(header=False)}")
    else:
        assert cfg.store_results, "You must store the reults to evaluate the metrics"
        df = do_not_evaluate_metric(task, cfg, algorithm_params, result_folder, runtime)
        log.info("Finished")
        log.info(f"Summary:\n{df.transpose().to_string(header=False)}")
    
if __name__ == "__main__":
    run_benchmark()
